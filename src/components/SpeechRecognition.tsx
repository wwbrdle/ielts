import React, { useState, useEffect, useRef } from 'react';
import './SpeechRecognition.css';

interface SpeechRecognitionProps {
  isRecording: boolean;
  onStartRecording: () => void;
  onStopRecording: () => void;
  onRecordingComplete: (transcript: string) => void;
  onTranscriptUpdate: (transcript: string) => void;
}

const SpeechRecognition: React.FC<SpeechRecognitionProps> = ({
  isRecording,
  onStartRecording,
  onStopRecording,
  onRecordingComplete,
  onTranscriptUpdate
}) => {

  const [isSupported, setIsSupported] = useState<boolean>(false);
  const recognitionRef = useRef<any>(null);

  useEffect(() => {
    // Web Speech API ì§€ì› í™•ì¸
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (SpeechRecognition) {
      setIsSupported(true);
      recognitionRef.current = new SpeechRecognition();
      recognitionRef.current.continuous = true;
      recognitionRef.current.interimResults = true;
      recognitionRef.current.lang = 'en-US';

      recognitionRef.current.onresult = (event: any) => {
        let finalTranscript = '';
        let interimTranscript = '';
        
        // event.resultsì˜ ëª¨ë“  ê²°ê³¼ë¥¼ ìˆœíšŒí•˜ë©´ì„œ transcriptë¥¼ êµ¬ì„±
        for (let i = 0; i < event.results.length; i++) {
          if (event.results[i].isFinal) {
            finalTranscript += event.results[i][0].transcript;
          } else {
            interimTranscript += event.results[i][0].transcript;
          }
        }
        
        // ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ì²´ transcriptë¥¼ ë¶€ëª¨ ì»´í¬ë„ŒíŠ¸ë¡œ ì „ë‹¬
        onTranscriptUpdate(finalTranscript + interimTranscript);
        
        // ìµœì¢… ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ onRecordingComplete í˜¸ì¶œ
        if (finalTranscript) {
          onRecordingComplete(finalTranscript);
        }
      };

      recognitionRef.current.onerror = (event: any) => {
        console.error('Speech recognition error:', event.error);
        if (event.error === 'no-speech') {
          alert('ìŒì„±ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
        }
      };
    }
  }, [onRecordingComplete, onTranscriptUpdate]);

  const handleStartRecording = () => {
    if (recognitionRef.current) {
      recognitionRef.current.start();
      onStartRecording();
    }
  };

  const handleStopRecording = () => {
    if (recognitionRef.current) {
      recognitionRef.current.stop();
      onStopRecording();
    }
  };

  if (!isSupported) {
    return (
      <div className="speech-recognition">
        <div className="not-supported">
          <p>âš ï¸ ì´ ë¸Œë¼ìš°ì €ì—ì„œëŠ” ìŒì„± ì¸ì‹ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</p>
          <p>Chrome, Edge, Safari ë“±ì˜ ìµœì‹  ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.</p>
        </div>
      </div>
    );
  }

  return (
    <div className="speech-recognition">
      <div className="recording-controls">
        {!isRecording ? (
          <button
            onClick={handleStartRecording}
            className="start-button"
          >
            ğŸ¤ ë…¹ìŒ ì‹œì‘
          </button>
        ) : (
          <button
            onClick={handleStopRecording}
            className="stop-button"
          >
            â¹ï¸ ë…¹ìŒ ì¤‘ì§€
          </button>
        )}
      </div>
      
      {isRecording && (
        <div className="recording-status">
          <div className="recording-indicator">
            <div className="pulse"></div>
            <span>ë…¹ìŒ ì¤‘... ë§ì”€í•´ì£¼ì„¸ìš”!</span>
          </div>
        </div>
      )}
    </div>
  );
};

export default SpeechRecognition;
